Notes from Gelman & Hill

## Chapter 18

`lm` function in R computes $\beta^{MLE} = (\textbf{X}^{T} \textbf{X})^{-1} \textbf{X} y$ which results in a best fit (least square of errors) for the parameters.

**Important conclusion: maximizing likelihood of the outputs given the inputs and the parameters is equivalent to minimizing the sum of squares error term**

**Weighted least squares ($\beta^{WLS}$):** Adds a W matrix for weighting each observation as having relatively more or less influence on the optimization:

$$\beta^{WLS} = (\textbf{X}^{T} W \textbf{X})^{-1} \textbf{X}^{T} W y$$

**Bayesian inference in a sentence:** Multiplying a prior distribution reflecting our beliefs by the maximimum likelihood learned from the data forms the posterior distribution, from which random draws reflect both inferences from the data and our beliefs.