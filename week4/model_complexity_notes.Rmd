---
title: "Model Complexity Notes"
output: pdf_document
---

`r`: The effective number of parameters

When N < M + 1  we know r < M + 1, r LTE N.

With a flat prior, K collapses to the Hat matrix.

`D` prior precision

posterior precision (Q) = prior precision (D) + q t(phi) phi

eff = Tr((D^-1 - Q^-1)D)
(Relative amount of learning; how much the uncertainty is reduced by learning)

effective features as being representative of the actual Data Generating Process (DGP)

#### Divergence and Model Approximation

**Kullback-Leibler Divergence:** How far away models are from each other (will be used to measure how my model is away from  the truth)

**w0:** least false parameter

The objective of the KL divergence: Get cross(relative)-entropy as close to entropy as possible. Minimize cross-entropy so that we are minimizing the divergence.

Estimate the distance with samples to find the w0.

Score the model in the family of parametric models to choose the one that makes the distance smallest.

Cross-entropy is like the negative scaled log likelihood

`g`: shrinkage of D (prior covariance), g*D - scalar on D, like overparameterizing, g = 1, doing this kind of shrinkage with the gaussian prior

`delta`: regularization term, eigenvalues all increased by delta

Zellner's g prior?

In generating samples, taking the integral to evaluate the sample mean I have samples that g generates, replacing known quantities from these samples

MLE is giving you the representative of the parametric family, estimated from the data

I want to find some way to estimate this distance, I want to compute the cross entropy term of the estimate against reality

Approximate the cross entropy with the sample average - this is the maximized log likelihood

How to pick the model

* M is the number of parameters estimating for this model (0 to D) (t is D dimensional)
* M is (w1, ..., wm, 0m+1, ..., 0D-m)

(slide 9) Transformation of maximized log likelihood is transformed with some bias, bigger the model bigger the bias, the more data the less bias
