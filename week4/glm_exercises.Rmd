---
title: Group 8 - GLM Exercises
date: November 10, 2015
output: pdf_document
---

## 1. A density that belongs in the one-parameter exponential family has the following (canonical) representation:

$p(z) = ...$

where $\theta$ is called the natural parameter, and c and h are functions whose exact form depends on the particular density. For many members of this family, q > 0, is a further unknown parameter, often called a precision parameter.

## 1. The following densities belong to the exponential family. Identify $\theta$, $q$ and an appropriate $c(\theta)$, for each of them (take into account that for some z might be a transformation of $t$):

### Normal

[ADD ME]

### Bernoulli

$t \textasciitilde Bern(n,p) = p^{z} (1-p)^{1-z}$

$= exp \Bigg\{ log(p^{z}) + log((1-p)^{1-z}) \Bigg\}$

$= exp \Bigg\{ z log(p) + (1-z) log(1-p) \Bigg\}$

$= exp \Bigg\{ z log(p) + log(1-p) - z log(1-p) \Bigg\}$

$= exp \Bigg\{ z log(\frac{p}{1-p}) + log(1-p) \Bigg\}$

**Result:**

$\theta = log(\frac{p}{1-p})$

$q = 1$

$c(\theta) = -log(1-p)$

### Binomial

$t \textasciitilde Bin(n,p) = {n \choose k} p^{k} (1-p)^{n-k}$

$= exp \Bigg\{ log({n \choose k }) + log(p^{k}) + log((1-p)^{n-k}) \Bigg\}$

$= exp \Bigg\{ log({n \choose k }) + k log(p) + (n-k) log(1-p) \Bigg\}$

$= exp \Bigg\{ log({n \choose k }) + k log(\frac{p}{1-p}) + n log(1-p) \Bigg\}$

$= exp \Bigg\{ n \Bigg[\frac{k}{n} log(\frac{p}{1-p}) + log(1-p)\Bigg] + log({n \choose k }) \Bigg\}$

**Result:**

$\theta = log(\frac{p}{1-p})$

$q = n$

$c(\theta) = log({n \choose k })$

### Poisson

$t \textasciitilde Poisson(\lambda) = \frac{\lambda^{k}e^{-\lambda}}{k!}$

$= exp \Bigg\{ log(\lambda^{k} e^{-\lambda}) - log(k!) \Bigg\}$

$= exp \Bigg\{ k*log(\lambda) -\lambda log(e) - log(k!) \Bigg\}$

$= exp \Bigg\{ k*log(\lambda) -\lambda - log(k!) \Bigg\}$

**Result:**

$\theta = log(\lambda)$

$q = 1$

$c(\theta) = -\lambda$


## 2. Identify the canonical link function for each of the models given above.

I JUST LOOKED THIS UP I'M NOT SURE IT'S CORRECT.

**Normal**

$\mu$

**Bernoulli**

$logit(\mu)$

**Binomial**

$logit(\mu)$

**Poisson**

$log(\mu)$

#### 3. Consider now the generalised linear model:

$t_{n}|x_{n} -> NdEF(\theta(x_{n},w),q\gamma_{n})$ with

$\theta(x_{n}, w) = (c')^{-1}(g^{-1}(\phi(x_{n})^{T} w)) =: f (\phi(x_{n})^{T} w)$


## 2. $R^{2}$ and deviance

**1. Show that for any linear regression model:**

$-2 log p(t|X, w_{MLE}, q_{MLE} ) = N log e^{T} e + const$

**where "const" does not depend on $M$ or $X$.**

We know that:

$-2logp(t|X,w,q) =-Nlogq+q(t-\phi w)^{T}(t-\phi w)+const$

Maximizing with respect to $w$ gives us:

$w_{mle} = (\phi^{T} \phi)^{-1}\phi^{T}t$

We plug this $w_{mle}$ back into the log likelihood to get the likelihood function given $w_{mle}$:

$-2logp(t|X,w_{mle},q) = -Nlog(q) + qe^{T}e$

Now, we can take the derivative wrt to $q$ to solve for $q_{mle}$ which gives us:

$q_{mle} = \Bigg(\frac{1}{N} e^{T}e\Bigg)^{-1}$

We use this to solve for the deviance of the maximum likelihood function:

$-2logp(t|X,w_{mle},q_{mle}) = -N log(q_{mle}) + q_{mle}e^{T}e$

$-N log(q_{mle}) + q_{mle}e^{T}e = -N log\Bigg((\frac{1}{N} e^{T}e)^{-1}\Bigg) + \Bigg(\frac{1}{N} e^{T}e\Bigg)^{-1}e^{T}e$

$= N log\Bigg((\frac{1}{N} e^{T}e)\Bigg) + N$

$= N[log(e^{T}e) - log(N)] + N$

$= Nlog(e^{T}e) - N(log(N) + 1)$

$= Nlog(e^{T}e) + const$

**2. Show that in the null model,**

**$w_{0,MLE} = \bar{t}$**

[ADD ME]

**3. The null model is nested within the saturated model, and it corresponds to the special case where w1 = · · · = wM = 0. Let D0 be the deviance of the null model and D1 be that of the saturated model. Show that:**

$D_{0} - D_{1} = -N log(1 - R^{2})$

**where $R^{2}$ is the coefficient $R^{2}$ for the saturated model.**

We know from **part 1** that:

$-2logp(t|X,w_{mle},q_{mle}) = Nlog(e^{T}e) + const$

Which also what we equate as $D_{M}$ for model $M$, so:

$D_{0} - D_{1} = Nlog(e_{0}^{T}e_{0}) - Nlog(e_{1}^{T}e_{1})$

$= N \Bigg[ log(\frac{e_{0}^{T}e_{0}}{e_{1}^{T}e_{1}}) \Bigg]$

We can use the property that $e = t - \hat{t}$

...
