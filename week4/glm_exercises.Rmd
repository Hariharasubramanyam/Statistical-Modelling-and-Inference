---
title: Group 8 - GLM Exercises
date: November 10, 2015
output: pdf_document
---

# 1. A density that belongs in the one-parameter exponential family has the following (canonical) representation:

$p(z) = ...$

where $\theta$ is called the natural parameter, and c and h are functions whose exact form depends on the particular density. For many members of this family, q > 0, is a further unknown parameter, often called a precision parameter.

#### 1. The following densities belong to the exponential family. Identify $\theta$, $q$ and an appropriate $c(\theta)$, for each of them (take into account that for some z might be a transformation of $t$):

### Normal

$t \textasciitilde N(\mu, -q) = exp \Bigg\{  \frac{-(t - \mu)^2}{2q} \Bigg\}$

$= exp \Bigg\{  \frac{-(t^2 - 2\mu t + \mu^2)}{2q} \Bigg\}$

$= exp \Bigg\{  -\frac{1}{2q} \Bigg[ t^2 - 2\mu t + \mu^2 \Bigg] \Bigg\}$

$= exp \Bigg\{  -\frac{1}{q} \Bigg[ \frac{t^2}{2} - \mu t + \frac{\mu^2}{2} \Bigg] \Bigg\}$

$= exp \Bigg\{  \frac{1}{q} \Bigg[ -\frac{t^2}{2} + \mu t - \frac{\mu^2}{2} \Bigg] \Bigg\}$

$= exp \Bigg\{  \frac{1}{q} \Bigg[ \mu t - \frac{\mu^2}{2} \Bigg] - \frac{t^2}{2q}\Bigg\}$

**Result:**

$\theta = \mu$

$q = q$ (since $q^{-1}$ is the variance parameter passed, than $q$ the paramter is the precision and the q above is the variance for the normal distribution)

$c(\theta) = \frac{\mu^2}{2}$

### Bernoulli

$t \textasciitilde Bern(n,p) = p^{z} (1-p)^{1-z}$

$= exp \Bigg\{ log(p^{z}) + log((1-p)^{1-z}) \Bigg\}$

$= exp \Bigg\{ z log(p) + (1-z) log(1-p) \Bigg\}$

$= exp \Bigg\{ z log(p) + log(1-p) - z log(1-p) \Bigg\}$

$= exp \Bigg\{ z log(\frac{p}{1-p}) + log(1-p) \Bigg\}$

**Result:**

$\theta = log(\frac{p}{1-p})$

$q = 1$

$c(\theta) = -log(1-p)$

### Binomial

$t \textasciitilde Bin(n,p) = {n \choose k} p^{k} (1-p)^{n-k}$

$= exp \Bigg\{ log({n \choose k }) + log(p^{k}) + log((1-p)^{n-k}) \Bigg\}$

$= exp \Bigg\{ log({n \choose k }) + k log(p) + (n-k) log(1-p) \Bigg\}$

$= exp \Bigg\{ log({n \choose k }) + k log(\frac{p}{1-p}) + n log(1-p) \Bigg\}$

$= exp \Bigg\{ n \Bigg[\frac{k}{n} log(\frac{p}{1-p}) + log(1-p)\Bigg] + log({n \choose k }) \Bigg\}$

**Result:**

$\theta = log(\frac{p}{1-p})$

$q = n$

$c(\theta) = log({n \choose k })$

### Poisson

$t \textasciitilde Poisson(\lambda) = \frac{\lambda^{k}e^{-\lambda}}{k!}$

$= exp \Bigg\{ log(\lambda^{k} e^{-\lambda}) - log(k!) \Bigg\}$

$= exp \Bigg\{ k*log(\lambda) -\lambda log(e) - log(k!) \Bigg\}$

$= exp \Bigg\{ k*log(\lambda) -\lambda - log(k!) \Bigg\}$

**Result:**

$\theta = log(\lambda)$

$q = 1$

$c(\theta) = -\lambda$


#### 2. Identify the canonical link function for each of the models given above.

**Normal**

$\mu$

**Bernoulli**

$logit(\mu)$

**Binomial**

$logit(\mu)$

**Poisson**

$log(\mu)$

#### 3. Consider now the generalised linear model:

$t_{n}|x_{n} -> NdEF(\theta(x_{n},w),q\gamma_{n})$ with

$\theta(x_{n}, w) = (c')^{-1}(g^{-1}(\phi(x_{n})^{T} w)) =: f (\phi(x_{n})^{T} w)$

We know the log-likehood of the NdEF family of distributions takes the form:

$2 log(p(t|x,\gamma,w,q)) = 2q\sum_{n} \gamma_{n} \Bigg[ t_{n} \theta(x_{n},w) - c(\theta(x_{n},w))  \Bigg]$

We will prove it is log-concave if it is the negative of a log-convex function,
 so will prove:

$f(\theta(x_{n},w),q\gamma_{n}) = -2q\sum_{n} \gamma_{n} \Bigg[ t_{n} \theta(x_{n},w) - c(\theta(x_{n},w))  \Bigg]$

is log-covex, taking into account the link function constructed as:

$g(y) = c'^{-1}(\theta) => \theta(x_{n},w) = \phi(x_{n}^{T} w$

We will prove it is log-convex by showing the second derivative is positive semi-definite for all $\theta(x_{n},w)$

The first derivative of the negative log likelihood is:

$f'(\theta(x_{n},w),q\gamma_{n}) = -2q\Bigg[ \sum_{n} \gamma_{n} t_{n} \phi(x_{n})^{T} - \sum_{n} \gamma_{n} c'(\phi(x_{n}^{T}w)) \Bigg]$

$f''(\theta(x_{n},w),q\gamma_{n}) = 2q\sum_{n} \gamma_{n} c''(\phi(x_{n})^{T}w)$

We know that $c''$ is the variance function and $\gamma{n}$ and $q$ are positive by definition. So the second derivative is always positive and therefore:

$x^{T}f''x \geq 0$ for all x.


# 2. $R^{2}$ and deviance

**1. Show that for any linear regression model:**

$-2 log p(t|X, w_{MLE}, q_{MLE} ) = N log e^{T} e + const$

**where "const" does not depend on $M$ or $X$.**

We know that:

$w_{mle} = (\phi^{T} \phi)^{-1}\phi^{T}t$

and

$q_{mle} = \Bigg(\frac{1}{N} e^{T}e\Bigg)^{-1}$

We use this to solve for the deviance of the maximum likelihood function:

$-2logp(t|X,w_{mle},q_{mle}) = -N log(q_{mle}) + q_{mle}e^{T}e$

$-N log(q_{mle}) + q_{mle}e^{T}e = -N log\Bigg((\frac{1}{N} e^{T}e)^{-1}\Bigg) + \Bigg(\frac{1}{N} e^{T}e\Bigg)^{-1}e^{T}e$

$= N log\Bigg((\frac{1}{N} e^{T}e)\Bigg) + N$

$= N[log(e^{T}e) - log(N)] + N$

$= Nlog(e^{T}e) - N(log(N) + 1)$

$= Nlog(e^{T}e) + const$

**2. Show that in the null model,**

**$w_{0,MLE} = \bar{t}$**

We know from **1**:

$w_{mle} = (\phi^{t}\phi)^{-1}\phi t$

In the case of the null model, $\phi$ is a vector of ones so this becomes:

$\frac{1}{N} \sum_{n=1}^{N} t_{n} = \bar{t}$ 


**3. The null model is nested within the saturated model, and it corresponds to the special case where $w_{1} = ... w_{M} = 0$. Let $D_{0}$ be the deviance of the null model and $D_{1}$ be that of the saturated model. Show that:**

$D_{0} - D_{1} = -N log(1 - R^{2})$

**where $R^{2}$ is the coefficient $R^{2}$ for the saturated model.**

We know from **part 1** that:

$-2logp(t|X,w_{mle},q_{mle}) = Nlog(e^{T}e) + const$

Which also what we equate as $D_{M}$ for model $M$, so:

$D_{0} - D_{1} = Nlog(e_{0}^{T}e_{0}) - Nlog(e_{1}^{T}e_{1})$

$= N log(\frac{e_{0}^{T}e_{0}}{e_{1}^{T}e_{1}})$

We use the following properties:

$e = t - \hat{t}$, and,

$R^{2} = 1 - \frac{\sum_{i} (t_{i} - \hat{t}_{i})^2}{\sum_{i} (t_{i} - \bar{t})^{2}}$

$1 - R^{2} = \frac{\sum_{i} (t_{i} - \hat{t}_{i})^2}{\sum_{i} (t_{i} - \bar{t})^{2}}$

To show that 

$= N log(\frac{e_{0}^{T}e_{0}}{e_{1}^{T}e_{1}})$

$= N log(\frac{\sum_{i} (t_{i} - \bar{t})^2}{\sum_{i} (t_{i} - \hat{t_{i}})^{2}})$

$= N log({\sum_{i} (t_{i} - \bar{t})^2}) - N log({\sum_{i} (t_{i} - \hat{t_{i}})^{2}})$

$= - N log(\frac{\sum_{i} (t_{i} - \hat{t_{i}})^{2}}{\sum_{i} (t_{i} - \bar{t})^2})$

$= -N log(1 - R^{2})$
