# Model Complexity

Pulling features in and out of the model.

One way to measure the cross-entropy is to estimate the bias

## Marginal Likelihood

Leveraging the machinery - measure model one and model two, compare 2 models, going to balance the model fit and how complexity.

Which of all these models is more interesting to me?

"one step of the conjugate gradient optimization algorithm"

The Bayesian/Schwartz IC approximation

* AIC will pick the bigger one (as N grows large)
* "Asymptotically correct"
* Motivates Bayesian information criteria

## "The real truth" / Barlett's paradox (the bad side of the coin)

Paradox (not really a paradox)

If you are considering 2 models of different dimension

Flatness of the prior has impact on the measurement result of complexity?

If you chose delta small enough

How the prior model affect the model posterior and the model comparison.

__Questions:__

* Why measure distance in terms of least false parameters and not the distance between estimates and observed or between models
* How do we calculate this -2 log marginal likelihood (-2log p_{m}(t))
