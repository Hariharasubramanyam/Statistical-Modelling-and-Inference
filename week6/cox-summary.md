0. 1/3-1/2 page reply to the following points:
* **What is the issue the article addresses?**

**Big data** always produces high estimates of **precision** because standard statistical techniques assume at most weak dependence.

> With very large amounts of data, direct use
of standard statistical methods, including simulation-based approaches, will tend to produce estimates of
apparently very high precision, essentially because of strong explicit or implicit assumptions of at most
weak dependence underlying such methods. (page 1)


**What do you think is the contribution?**

The paper presents a model of variance that depends on a rate of increase.

The paper formulizes how error may grow in relation to the size of the data. This work contributes towards intruducing a model for main sources of variation.

Generalizes the impact of various and growing sources of variation on precision in statistical quantities such as mean and coefficients.

**How would you go about modelling Big Data?**

I would subset the data to a workable size.

**What do you think is the biggest danger in analysing Big Data?**

* Conventional statistics may be practically difficult
* Assumptions in standard statisical techniques may drive misleading results (precision of estimates topically)
* If data is analyzed sequentially (as opposed to all at once) the entry of new sources of variance 

**In Cox's words, you find the 2 words "Big Data", "frightening" or "beautiful"?**

> Contributions on different time scales are regarded as statistically independent