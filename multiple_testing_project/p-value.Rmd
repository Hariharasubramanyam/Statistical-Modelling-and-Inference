---
title: "Multiple Testing Project: p-value"
output: pdf_document
---

# 2.2 P-value

## 1. Distribution of the p-value under the null

**1.1 Show that for any $\alpha$, $c_{\alpha} = F_{H}^{-1}(1 - \alpha)$**

We know that $\alpha = 1 - F_{H}(c_{\alpha})$ from the definition of $c_{\alpha}$.

$\alpha = 1 - F_{H}(c_{\alpha})$

$\alpha - 1 = - F_{H}(c_{\alpha})$

$-(\alpha - 1) = F_{H}(c_{\alpha})$

$1 - \alpha = F_{H}(c_{\alpha})$

$F^{-1}_{H}(1 - \alpha) = c_{\alpha}$

$c_{\alpha} = F^{-1}_{H}(1 - \alpha)$

Q.E.D.

**1.2 Show that the p-value of the test, as a function of the data X used, is given by $p(\textbf{X}) = 1 - F_{H}(T(\textbf{X}))$.**

The p-value is defined as $p-value = inf\{\alpha : T(X) \in R_{\alpha}\}$

Which is to say that the p-value is the *smallest* $\alpha$ for which $T(X)$ is in the region $R_{\alpha}$ of the probability distribution $P_{H}$

So the p-value is an instance of $\alpha$, which is defined as $\alpha = 1 - F_{H}(c_{\alpha})$ where $c_{\alpha}$ is chosen so that the equation is true. Therefore, if we replace $c_{\alpha}$ with our test statistic $T(\textbf{X})$, we get a $p(\textbf{X}) = 1 - F_{H}(\textbf{X})$.

This matters because it highlights that the choice of $\alpha$ sets the minimum p-value for $\textbf{X}$ as $p(\textbf{X})$ such that one rejects the hypothesis that X is from the same distribution as Y when $T(\textbf{X}) > c_{\alpha}$.

From the previous question we know, 
$F_{H}(c_{\alpha}) = 1 - \alpha$

$F_{H}(c_{\alpha}) = 1 - P_{H}[T(\textbf{X}) > c_{\alpha}]$

$P_{H}[T(\textbf{X}) > c_{\alpha}] = 1 - F_{H}(c_{\alpha})$

$P_{H}[T(\textbf{X}) > \textbf{x}] = 1 - F_{H}(\textbf{X})$

$p(\textbf{X}) = 1 - F_{H}(\textbf{X})$


**1.3 Show that for any univariate random variable $y$ with continuous distribution function $F$, the random variables $F(y)$ and $1 - F(y)$ follow the uniform distribution.**

[resource](http://math.stackexchange.com/questions/868400/showing-that-y-has-a-uniform-distribution-if-y-fx-where-f-is-the-cdf-of-x)
&nbsp;
[resource](https://people.math.ethz.ch/~embrecht/ftp/generalized_inverse.pdf)

We know a function to be uniformly distributed if it has constant probability for any input value in the range for which it is defined. Say $y_{i}$ is some value for which $F_{Y}(y_{i})$ is defined. The set of all $y_{i}$ is denoted as $\textbf{Y}$ and the probability of $F_{Y}(\textbf{Y})$ is given by $P[F_{Y}(\textbf{Y})]$.

$F_{Y}$ is uniformly distributed when $P[F_{Y}(y_{1})] = P[F_{Y}(y_{2})]$ where $y_{1}$ and $y_{2}$ are 2 distinct values in $\textbf{Y}$.

$$P[F_{Y}(\textbf{Y}) \leq y] = P[F^{-1}(F_{Y}(\textbf{Y})) \leq F^{-1}(y)] = P[\textbf{Y} \leq F^{-1}(y)]$$

We know that $F_{Y}$ is strictly increasing $F^{-1}_{Y}(F_{Y}(\textbf{Y})) = y$ by the definition of the definition of a quantile function which is the generalized inverse of a CDF. We use this property to concolude:

$$P[\textbf{Y} \leq F^{-1}_{Y}(y)] = F_{Y}(F^{-1}_{Y}(y)) = y$$

This is true for all values of $\textbf{Y}$.

By definition if a function $F_{Y}$ follows the uniform distribution, so does $1 - F_{Y}$

**1.4 Using the above results, show that the p-value follows the uniform distribution under H.**

$\alpha = 1 - F_{H}(c_{\alpha})$

$c_{\alpha}$ being a continuous random variable, we know $F_{H}(c_{\alpha})$ to have a uniform distribution. If $F_{H}(c_{\alpha})$ follows a uniform distribution, $1 - F_{H}(c_{\alpha})$ does too.

*Doubts*

* Can we prove if $F_{H}$ is uniform, so is $1 - F_{H}$?
* How do we get from $P[\textbf{Y} \leq F^{-1}_{Y}(y)] = F_{Y}(F^{-1}_{Y}(y))$ ?


