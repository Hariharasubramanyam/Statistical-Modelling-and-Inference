---
title: "Multiple Testing Project: Multiple Testing under independence assumptions"
output: pdf_document
---

##### MT under independence assumptions

**2.1 Show that $P_{H}[\cap^{m}_{i = 1} \{ y_{i} > \alpha \} ] = (1 - \alpha)^{m}$**

As $y_{1},..., y_{m}$ are independent unifrom random variables, the joint distribution of the probability that for every $y_{i} > \alpha$ is nothing but the product of the probabilities,

$$P_{H}[\cap^{m}_{i = 1} \{ y_{i} > \alpha \} ] = \prod_{i=1}^{m}P(y_{i} > \alpha)$$

We know from the previous exercises that $P(y_{i} < \alpha) = \alpha$, or, equivalently, $P(y_{i} > \alpha) = 1 - \alpha$. Therefore,

$$\prod_{i=1}^{m}P(y_{i} > \alpha) = (1 - \alpha)^{m}$$

**2.2 Show that the probability of rejecting at significance level $\alpha$ at least one of the independent tests is $1 - ( 1- \alpha)^{m}$.**

The probability of rejecting at least one of the tests could be expressed as the union,

$P_{H}[\cup^{m}_{i = 1} \{ y_{i} < \alpha \} ]$

And we know that the probability of at least one rejection is the complement of the probability of having no rejections, which we have already determined as being equal to $(1 - \alpha)^{m}$. Thus,

$P_{H}[\cup^{m}_{i = 1} \{ y_{i} < \alpha \} ] = 1 - P_{H}[\cap^{m}_{i = 1} \{ y_{i} > \alpha \} ]$

$P_{H}[\cup^{m}_{i = 1} \{ y_{i} < \alpha \} ] = 1 - (1 - \alpha)^{m}$

**2.3 If we want the overall type I error to be $\alpha$, each test should be rejected at a certain probability $p_{r}$ that satisfies:**

$P_{H}[\cup^{m}_{i = 1} \{ y_{i} < p_{r} \} ] = \alpha$

$1 - (1 - p_{r})^{m} = \alpha$

$p_{r} = 1 - (1 - \alpha)^{\frac{1}{m}}$

**2.4 $f_{1}(\alpha) = 1 - (1 - \alpha)^{1/m}, \ f_{2}(\alpha) = \alpha/m, \ f_{3}(\alpha) = \alpha. \ for \  \alpha \in [0,1]$. Check that $f_{2} \leqslant f_{1} \leqslant f_{3}$. **

```{r, echo=FALSE}
m <- 10
f1 <- function(alpha) 1 - (1- alpha)**(1/m)
f2 <- function(alpha) alpha/m
f3 <- function(alpha) alpha

plot(f1)
curve(f2, add=TRUE, col='red')
curve(f3, add=TRUE, col= 'blue')
```

**2.5**
(optional)
I think its doable, playing with the function's derivatives.

**2.6**
Don't know what are we supposed to do here.

#### A conservative but robust test: Bonferroni

**3. Upper-bounding the probability of at least one rejection.** 
Show that $\alpha \leqslant P_{C - H}[\cup^{m}_{i=1} \{ p_{i}(Y_{i}) < \alpha \}] \leqslant m \alpha$

Using Boole's inequality we can say that:

$P_{C - H}[\cup^{m}_{i=1} \{ p_{i}(Y_{i}) < \alpha \}] \leqslant \sum^{m}_{i=1} P(p_{i}(Y_{i}) < \alpha)$

From section 1 of the project we know that $P(P(p_{i}(Y_{i}) < \alpha) = \alpha$. Therefore, $\sum^{m}_{i=1} P(p_{i}(Y_{i}) < \alpha) = m \alpha$.

Hence, we have proved the upper bound $P_{C - H}[\cup^{m}_{i=1} \{ p_{i}(Y_{i}) < \alpha \}] \leqslant m \alpha$.

The lower bound of $\alpha$ turns into an equality at $P_{C - H}[\cup^{m}_{i=1} \{ p_{i}(Y_{i}) < \frac{\alpha}{m} \}]$, since, solving for $p_{size}$,

$P_{C - H}[\cup^{m}_{i=1} \{ p_{i}(Y_{i}) < p_{size} \}] \leqslant \sum^{m}_{i=1} P(p_{i}(Y_{i}) < p_{size}) = \alpha$

$m p_{size} = \alpha$

$p_{size} = \frac{\alpha}{m}$

Using a threshold probability of $\frac{\alpha}{m}$ for rejecting each individual test ensures that the overall type I error equals $\alpha$. 

Now, if $\alpha = P_{C - H}[\cup^{m}_{i=1} \{ p_{i}(Y_{i}) < \frac{\alpha}{m} \}]$, for any $p_{size} > \frac{\alpha}{m}$ such as $p_{size} = \alpha$, $\alpha \leqslant P_{C - H}[\cup^{m}_{i=1} \{ p_{i}(Y_{i}) < p_{size} \}]$, proving the lower bound. 

##### Ordered p-values, family-wise error rate and a new MT correction

**4.**
If we reject test $i$ when $p_{(i)} < \frac{i \alpha}{m}$, the probability of at least one false rejection is:

$P_{H}[\cup^{m}_{i = 1} \{ y_{i} < \frac{i \alpha}{m} \} ]$

$P_{H}[\cup^{m}_{i = 1} \{ y_{i} < \frac{i \alpha}{m} \} ] = 1 - P_{H}[\cap^{m}_{i = 1} \{ y_{i} > \frac{i \alpha}{m} \} ]$

$1 - P_{H}[\cap^{m}_{i = 1} \{ y_{i} > \frac{i \alpha}{m} \} ] = 1 - (1 - \alpha)^{m} = 1 - (1 - \frac{i \alpha}{m})^{m}$








