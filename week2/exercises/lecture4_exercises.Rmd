---
title: "Lecture 4 Exercises"
output: html_document
---

**1. Continuation of your work on smooth function estimation with the data in curve_data.txt.**

```{r}
library(MASS)

M = 9
basis_type = 'Gauss'
delta = 1.0
q = (1/0.1)**2
data <- read.table("curve_data.txt")
test.x <- seq(0,1,1/1000)

precision.bayes <- function(data, x, M, basis_type, delta, q) {
  dev.off()
  input_data_params <- post.params(data, M, basis_type, delta, q)
  Qbayes <- input_data_params[[1]]
  wbayes <- input_data_params[[2]]
  
  # create phi(testx) by sending all the test x to phix
  phi.test.x <- matrix(nrow = length(test.x), ncol = M+1)
  for (n in 1:length(test.x)) {
    phi.test.x[n,] <- c(1, phix(test.x[n], M, basis_type))
  }
  
  test.y <- matrix(nrow = length(test.x), ncol = 2)
  dimnames(test.y)[[2]] <- list("test.x", "test.y")
  for (n in 1:nrow(phi.test.x)) {
    test.y[n,"test.x"] <- test.x[n]
    test.y[n,"test.y"] <- t(phi.test.x[n,]) %*% wbayes
  }

  sds <- rep(0, nrow(phi.test.x))
  Qbayesinv <- solve(Qbayes)
  for (row_idx in 1:nrow(phi.test.x)) {
    sds[row_idx] <- sqrt(t(phi.test.x[row_idx,]) %*% Qbayesinv %*% phi.test.x[row_idx,])
  }

  return(list(test.y, sds, Qbayes))
}

result <- precision.bayes(data, x, M, basis_type, delta, q)
test.ys <- result[1][[1]]
sds <- result[2][[1]]
Qbayes <- result[3][[1]]

plot(data$x, data$t, ylim = range(-1.5:1.5), col = 'red')
lines(x = test.ys[,"test.x"], y = test.ys[,"test.y"])
lines(x = test.ys[,"test.x"], y = (test.ys[,"test.y"] + sds), col = 'grey')
lines(x = test.ys[,"test.x"], y = (test.ys[,"test.y"] - sds), col = 'grey')

plot(data, col = 'red')
nsims <- 100
mins <- rep(0, nsims)
maxs <- rep(0, nsims)
for (sim in 1:nsims) {
  # generate random draws of N(w,cov) and plot random xs evaluated for each
  # Procedure: 
  #  1) generate a random draw of N(w, cov) -> this becomes a new (random) w (from the same distribution as wbayes)
  wbayesrand <- mvrnorm(n = 1, wbayes, Sigma = solve(Qbayes))
  #  2) generate 100 random xs to calculate phix(x...)
  xs <- runif(100)
  #  3) generate their y's as t(phix(xs)) %*% wbayesrand
  ys <- rep(0, length(xs))
  for (n in 1:length(xs)) {
    testphi <- c(1, phix(xs[n], M, basis_type))
    ys[n] <- t(testphi) %*% wbayesrand
  }
  mins[sim] <- min(ys)
  maxs[sim] <- max(ys)
  lines(predict(splines::interpSpline(xs, ys)), col ='grey')
}
points(data, col ='red')

observed_min <- min(data$t)
observed_max <- max(data$t)

scatterhist <- function(x, y, x1, y1, xlab="", ylab=""){
  zones=matrix(c(2,0,1,3), ncol=2, byrow=TRUE)
  layout(zones, widths=c(4/5,1/5), heights=c(1/5,4/5))
  xhist = hist(x, plot=FALSE)
  yhist = hist(y, plot=FALSE)
  top = max(c(xhist$counts, yhist$counts))
  par(mar=c(3,3,1,1))
  plot(x,y, col = 'gray', pch = 16)
  points(x1, y1, col = 'red', pch = 16, cex = 2)
  par(mar=c(0,3,1,1))
  barplot(xhist$counts, axes=FALSE, ylim=c(0, top), space=0, col = 'dodgerblue')
  par(mar=c(3,0,1,1))
  barplot(yhist$counts, axes=FALSE, xlim=c(0, top), space=0, horiz=TRUE, col = 'dodgerblue')
  par(oma=c(3,3,0,0))
  mtext(xlab, side=1, line=1, outer=TRUE, adj=0, 
    at=.8 * (mean(x) - min(x))/(max(x)-min(x)))
  mtext(ylab, side=2, line=1, outer=TRUE, adj=0, 
    at=(.8 * (mean(y) - min(y))/(max(y) - min(y))))
}

scatterhist(mins, maxs, observed_min, observed_max)
```

**2.1 Show that the mean of the predictive distribution at an input location x, $φ(x)^{T} w_{Bayes}$ can be represented as $$\sum_{n=1}^{N} qφ(x)^{T} Q^{−1}φ(x_{n})t_{n}$$ therefore, as a linear combination of the training outputs $t_{n}$.**

We know that the predictive distribution is given by (slide 3):

$t_{N+1}|t,X,x_{N+1} ∼ N(φ(x_{N+1})^{T}w_{Bayes},φ(x_{N+1})^{T}Q^{−1}φ(x_{N+1})+q^{−1})$

From this we know the mean of the predictive distribution at **x** is: $φ(\textbf{x})^{T}w_{Bayes}$

and the equation is given by $w_{Bayes} = qQ^{−1}φ^{T}t$

At **x**, $φ^{T}t$ is $$\sum_{n=1}^{N} φ(x_{n})t_{n}$$

So we get the final result that the mean of the predictive distribution at **x** is:

$$\sum_{n=1}^{N} q φ(x)^{T} Q^{-1} φ(x_{n})t_{n}$$

**2.2 The weights above is a quantification of the similarity (in feature space) of the test input x and the training inputs xn. In particular, let $k(x, y) := qφ(x)^{T}Q^{−1}φ(y)$ Then, show that the weight of $t_{n}$ is $k(x,x_{n})$**

We know from 2.1 that the mean of the predictive distribution is given by:

$$\bar{x} = \sum_{n=1}^{N} q φ(x)^{T} Q^{-1} φ(x_{n})t_{n}$$

We use this to solve for $t_{n}$ and get

$\dfrac{\bar{x}}{\sum_{n=1}^{N} q φ(x)^{T} Q^{-1} φ(x_{n})t} = t_{n}$

We find the summation in the denominator acts as a weight of the mean at **x**.

**3. ** IS THIS RIGHT?

We know that:

$k(x, y) := qφ(x)^{T} Q^{−1}φ(y)$

So we can exchange x and y for $x_{n}$ and $x_{k}$

$k(x_{n}, x_{y}) := qφ(x_{n})^{T} Q^{−1}φ(x_{k})$

The matrix produced for all n and all k, e.g. every row of x (n) and every column of x (k) is exactly the matrix K:

$qφ(x_{n})^{T} Q^{−1}φ(x_{k}) = qΦQ^{−1}Φ^{T} = K$




**4. Notice that, by expanding Q,**

$K=q\phi(\delta I+q\phi^{T}\phi)^{−1}\phi^{T}  = \phi(\lambda I+\phi^{T}\phi)^{−1}\phi^{T}$.

**Show that when $\lambda = 0$, and provided $\phi^{T}\phi$ is invertible, K is precisely the “hat” matrix of linear regression. Therefore, the matrix of kernel weights provides a Bayesian version of such matrix. We will revisit this later in the course.**

The "Hat" matrix from linear regression is given by:

$H = \phi(\phi^{T}\phi)^{−1}\phi^{T}$

When $\lambda = 0$, the first term in paranthesis:

$K = \phi(\lambda I+\phi^{T}\phi)^{−1}\phi^{T}$

is a matrix of zeroes and thus when multiplied by $\phi$ is still a matrix of zeros so we remove it from the equation and the equation reduces to:

$K = H = \phi(\phi^{T}\phi)^{−1}\phi^{T}$
