---
title: "Lecture 4 Exercises"
output: html_document
---

**Continuation of your work on smooth function estimation with the data in curve_data.txt.**

```{r}
# install.packages('mvtnorm')
# Write a function that takes as input a vector of prediction input values, and returns the mean and the precision of the predictive distribution at each of these inputs.
fitted.bayes <- function(inputs, training_data) {
  # create a wbayes result from the training data
  
  # calculate mean and variance for each of the inputs using wbayes
  
}

```

**2.

We know that "the prior Gaussian random functions using Bayesian learning with training data (t, X) are updated to posterior Gaussian random funcs, such that:" (Bayesian prediction slide 6)

$y(x)|t, X ∼ N (φ(x)^{T} w_{Bayes} , φ(x)^{T} Q^{-1} φ(x))$

Since this is normally distributed we know the mean is equal to the expected value:

$E[y(x_{n}|t,X)] = \int x_{n} pdf(y(x_{n}|t,X)) dx$

**4. Notice that, by expanding Q,**

$K=q\phi(\delta I+q\phi^{T}\phi)^{−1}\phi^{T}  = \phi(\lambda I+\phi^{T}\phi)^{−1}\phi^{T}$.

**Show that when $\lambda = 0$, and provided $\phi^{T}\phi$ is invertible, K is precisely the “hat” matrix of linear regression. Therefore, the matrix of kernel weights provides a Bayesian version of such matrix. We will revisit this later in the course.**

The "Hat" matrix from linear regression is given by:

$H = \phi(\phi^{T}\phi)^{−1}\phi^{T}$

When $\lambda = 0$, the first term in paranthesis:

$K = \phi(\lambda I+\phi^{T}\phi)^{−1}\phi^{T}$

is a matrix of zeroes and thus when multiplied by $\phi$ is still a matrix of zeros so we remove it from the equation and the equation reduces to:

$K = H = \phi(\phi^{T}\phi)^{−1}\phi^{T}$
