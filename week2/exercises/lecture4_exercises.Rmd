---
title: "Lecture 4 Exercises"
output: html_document
---

**1. Continuation of your work on smooth function estimation with the data in curve_data.txt.**

```{r}
library(MASS)

M = 9
basis_type = 'Gauss'
delta = 1.0
q = (1/0.1)**2
data <- read.table("curve_data.txt")
x <- seq(0,1,1/1000)

precision.bayes <- function(data, x, M, basis_type, delta, q) {
  input_data_params <- post.params(data, M, basis_type, delta, q)
  Qbayes <- input_data_params[[1]]
  wbayes <- input_data_params[[2]]

  phi = phix(x[1], M, basis_type)
  for (i in 2:length(x)) {
    phi_ <- phix(x[i], M, basis_type)
    phi = rbind(phi, phi_)  
  }
  # Not sure if we should do this, just makes the vector multiplication work...
  phi <- cbind(rep(1, nrow(phi)), phi)

  means <- rep(0, nrow(phi))
  for (row_idx in 1:nrow(phi)) {
    means[row_idx] <- t(phi[row_idx,]) %*% wbayes
  }
  
  sds <- rep(0, nrow(phi))
  Qbayesinv <- solve(Qbayes)
  for (row_idx in 1:nrow(phi)) {
    sds[row_idx] <- sqrt(t(phi[row_idx,]) %*% Qbayesinv %*% phi[row_idx,])
  }

  return(list(means, sds, Qbayes))
}

result <- precision.bayes(data, x, M, basis_type, delta, q)
means <- result[1][[1]]
sds <- result[2][[1]]
sigma <- result[3][[1]]

plot(data$x, data$t, ylim = range(-1.5:1.5), col = 'red')
lines(predict(splines::interpSpline(x,means)))
lines(predict(splines::interpSpline(x,lower_bound)), col = 'grey')
lines(predict(splines::interpSpline(x,upper_bound)), col = 'grey')

# needs to be the means of the variables...
draws <- mvrnorm(100, mu = wbayes, Sigma = sigma)
dim(draws)

plot(data$x, data$t, ylim = range(-1.5:1.5))
for (i in 1:nrow(draws)){
  lines(predict(splines::interpSpline(x, draws[i,])))
}
dev.off()
mins <- apply(draws, 1, min)
maxs <- apply(draws, 1, max)

observed_min <- min(data$t)
observed_max <- max(data$t)

scatterhist <- function(x, y, x1, y1, xlab="", ylab=""){
  zones=matrix(c(2,0,1,3), ncol=2, byrow=TRUE)
  layout(zones, widths=c(4/5,1/5), heights=c(1/5,4/5))
  xhist = hist(x, plot=FALSE)
  yhist = hist(y, plot=FALSE)
  top = max(c(xhist$counts, yhist$counts))
  par(mar=c(3,3,1,1))
  plot(x,y, col = 'gray', pch = 16)
  points(x1, y1, col = 'red', pch = 16, cex = 2)
  par(mar=c(0,3,1,1))
  barplot(xhist$counts, axes=FALSE, ylim=c(0, top), space=0, col = 'dodgerblue')
  par(mar=c(3,0,1,1))
  barplot(yhist$counts, axes=FALSE, xlim=c(0, top), space=0, horiz=TRUE, col = 'dodgerblue')
  par(oma=c(3,3,0,0))
  mtext(xlab, side=1, line=1, outer=TRUE, adj=0, 
    at=.8 * (mean(x) - min(x))/(max(x)-min(x)))
  mtext(ylab, side=2, line=1, outer=TRUE, adj=0, 
    at=(.8 * (mean(y) - min(y))/(max(y) - min(y))))
}

scatterhist(mins, maxs, observed_min, observed_max)
```

**2.1 Show that the mean of the predictive distribution at an input location x, $φ(x)^{T} w_{Bayes}$ can be represented as $$\sum_{n=1}^{N} qφ(x)^{T} Q^{−1}φ(x_{n})t_{n}$$ therefore, as a linear combination of the training outputs $t_{n}$.**

We know that the predictive distribution is given by (slide 3):

$t_{N+1}|t,X,x_{N+1} ∼ N(φ(x_{N+1})^{T}w_{Bayes},φ(x_{N+1})^{T}Q^{−1}φ(x_{N+1})+q^{−1})$

From this we know the mean of the predictive distribution at **x** is: $φ(\textbf{x})^{T}w_{Bayes}$

and the equation is given by $w_{Bayes} = qQ^{−1}φ^{T}t$

At **x**, $φ^{T}t$ is $$\sum_{n=1}^{N} φ(x_{n})t_{n}$$

So we get the final result that the mean of the predictive distribution at **x** is:

$$\sum_{n=1}^{N} q φ(x)^{T} Q^{-1} φ(x_{n})t_{n}$$

**2.2 The weights above is a quantification of the similarity (in feature space) of the test input x and the training inputs xn. In particular, let $k(x, y) := qφ(x)^{T}Q^{−1}φ(y)$ Then, show that the weight of $t_{n}$ is $k(x,x_{n})$**

We know from 2.1 that the mean of the predictive distribution is given by:

$$\bar{x} = \sum_{n=1}^{N} q φ(x)^{T} Q^{-1} φ(x_{n})t_{n}$$

We use this to solve for $t_{n}$ and get

$\dfrac{\bar{x}}{\sum_{n=1}^{N} q φ(x)^{T} Q^{-1} φ(x_{n})t} = t_{n}$

We find the summation in the denominator acts as a weight of the mean at **x**.

**3. ** IS THIS RIGHT?

We know that:

$k(x, y) := qφ(x)^{T} Q^{−1}φ(y)$

So we can exchange x and y for $x_{n}$ and $x_{k}$

$k(x_{n}, x_{y}) := qφ(x_{n})^{T} Q^{−1}φ(x_{k})$

The matrix produced for all n and all k, e.g. every row of x (n) and every column of x (k) is exactly the matrix K:

$qφ(x_{n})^{T} Q^{−1}φ(x_{k}) = qΦQ^{−1}Φ^{T} = K$




**4. Notice that, by expanding Q,**

$K=q\phi(\delta I+q\phi^{T}\phi)^{−1}\phi^{T}  = \phi(\lambda I+\phi^{T}\phi)^{−1}\phi^{T}$.

**Show that when $\lambda = 0$, and provided $\phi^{T}\phi$ is invertible, K is precisely the “hat” matrix of linear regression. Therefore, the matrix of kernel weights provides a Bayesian version of such matrix. We will revisit this later in the course.**

The "Hat" matrix from linear regression is given by:

$H = \phi(\phi^{T}\phi)^{−1}\phi^{T}$

When $\lambda = 0$, the first term in paranthesis:

$K = \phi(\lambda I+\phi^{T}\phi)^{−1}\phi^{T}$

is a matrix of zeroes and thus when multiplied by $\phi$ is still a matrix of zeros so we remove it from the equation and the equation reduces to:

$K = H = \phi(\phi^{T}\phi)^{−1}\phi^{T}$
