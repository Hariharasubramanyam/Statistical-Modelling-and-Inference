---
title: "Lecture 3 Exercises"
output: html_document
---

**1. For a Bayesian linear regression model with prior on $p(w) = N (w | μ, D−1)$ and q known, show that $w_{Bayes}$ solves $(D + q\phi^{T}\phi)w_{Bayes} = q\phi^{T}t + Dμ$**

To solve for bayes we take the known posterior probability:

$−2 log p(w|t,X) = −2qt^{T}\phi w + qw^{T}\phi^{T}\phi w + (w − μ)^{T}D(w − μ) + const$

To get the best estimate of w, we take the derivative of the posterior probability and set it equal to zero:

$\partial −2 log p(w|t, X)/\partial w$ = 0

To take the derivative of the first term:

$\partial −2qt^{T}\phi w/\partial w = −2qt^{T}\phi$

The derivative of the second term uses the property $\partial x^{T}Ax/ \partial x = 2Ax$

$\partial q w^{T} \phi^{T} \phi w/\partial w = 2q\phi^{T}\phi w$

Expanding the third term $(w − μ)^{T}D(w − μ)$ gives:

$= (w^{T}D - μ^{T}D)(w - μ)$

$= w^{T}Dw - μ^{T}Dw - w^{T}Dμ + μ^{T}Dμ$

$\partial (w^{T}Dw - μ^{T}Dw - w^{T}Dμ + μ^{T}Dμ) / \partial w = 2w^{T}D - 2μ^{T}D$

So we have:

$0 = -2qt^{T}\phi + 2qw^{T}\phi^{T}\phi + 2w^{T}D - 2μ^{T}D$

Move negative terms to the LHS and divide both sides by 2:

$qt^{T}\phi + μ^{T}D = qw^{T}\phi^{T}\phi + w^{T}D$

$qw^{T}\phi^{T}\phi + w^{T}D = qt^{T}\phi + μ^{T}D$

Take the transpose of both sides (note: $D^{T} = D$:

$q\phi^{T}\phi w + Dw = q\phi^{T}t + Dμ$

$(D + q\phi^{T}\phi)w_{Bayes} = q\phi^{T}t + Dμ$



**2. Curve fitting (pt1) The aim is to learn a smooth function from the cloud of points stored in curve_data.txt using Bayesian linear regression models. In all that follows the prior p(w) = N (w|0, δ−1I) is used and q, δ are constants specified by the user.**

1. Plot the data

```{r, echo = FALSE}
curve_data <- read.csv('curve_data.txt', sep = " ")
plot(curve_data)
```

2. Write a function in R, called phix...

```{r}
phix <- function(x, M, basis) {
  # Initialize vector to return
  u <- rep(0, M+1)
  # μs for Gauss
  mus <- rep(0, M+1)
  interval <- 1 / M

  if (basis == 'poly') {
    # evaluate x at each value of p, e.g. x^0, x^1, ..., x^M
    for (p in 0:M) u[p+1] <- x^p
  } else if (basis == 'Gauss') {
    # Populate μs with M intervals in [0,1]
    for (n in 1:M) mus[n+1] = mus[n] + interval
    # Generate Gaussian Kernels
    for (p in 0:M) u[p+1] <- exp((-(x - mus[p+1])^2)/0.1)
  }

  u
}
# Test: round(phix(0.3,4,'poly'), digits = 4) == c(1, 0.3, 0.09, 0.027, 0.0081)
# Test: plot(phix(0.3,10,'Gauss'))
```

3. Write a function in R, called post.params, that takes as input the training data, M, the type of basis, the function phix, $\delta$ and q and returns the parameters of the posterior distribution, $w_{Bayes}$ and $Q$.

```{r}
library(MASS)

post.params <- function(training_data, M, basis_type, phix_func, delta, q) {
  # regularization coefficient
  # training_data = curve_data
  # q = (1/0.1)^2
  # delta = 2.0
  # basis_type = 'Gauss'
  lambda = delta / q

  # Note: this requires the training data to have an x column
  xs = training_data$x
  # M = 9
  phi <- matrix(nrow = length(xs), ncol = M + 1)
  # Construct a phi matrix that is phix for each value of phix(x, M, 'Gauss') and append to phi matrix
  for (i in 1:(length(xs))) {
    phi[i,] = phix(xs[i], M, basis_type)
  }
  t = training_data$t
  
  # Slide 10 of Bayessian_regression.pdf
  w_Bayes = ginv(lambda * diag(M+1) + t(phi) %*% phi) %*% t(phi) %*% training_data$t

  #D = delta * diag(M)
  #Q = D + q * (t(phi) %*% phi)

  # FIXME: RETURN Q?
  phi %*% w_Bayes
}

M = 9
delta = 2.0
q = (1/0.1)^2
plot(curve_data, col = 'red')

result3 <- post.params(curve_data[1:10,], M, 'poly', 'phix_func', delta, q)
# Don't know how to smooth this out :(
lines(y = result3, x = curve_data$x, type = 'l', col = 'blue')
```
