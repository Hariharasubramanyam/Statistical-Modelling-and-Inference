---
title: "Lecture 3 Exercises"
output: html_document
---

**1. For a Bayesian linear regression model with prior on $p(w) = N (w | μ, D−1)$ and q known, show that $w_{Bayes}$ solves $(D + q\phi^{T}\phi)w_{Bayes} = q\phi^{T}t + Dμ$**

To solve for bayes we take the known posterior probability:

$−2 log p(w|t,X) = −2qt^{T}\phi w + qw^{T}\phi^{T}\phi w + (w − μ)^{T}D(w − μ) + const$

To get the best estimate of w, we take the derivative of the posterior probability and set it equal to zero:

$\partial −2 log p(w|t, X)/\partial w$ = 0

To take the derivative of the first term:

$\partial −2qt^{T}\phi w/\partial w = −2qt^{T}\phi$

The derivative of the second term uses the property $\partial x^{T}Ax/ \partial x = 2Ax$

$\partial q w^{T} \phi^{T} \phi w/\partial w = 2q\phi^{T}\phi w$

Expanding the third term $(w − μ)^{T}D(w − μ)$ gives:

$= (w^{T}D - μ^{T}D)(w - μ)$

$= w^{T}Dw - μ^{T}Dw - w^{T}Dμ + μ^{T}Dμ$

$\partial (w^{T}Dw - μ^{T}Dw - w^{T}Dμ + μ^{T}Dμ) / \partial w = 2w^{T}D - 2μ^{T}D$

So we have:

$0 = -2qt^{T}\phi + 2qw^{T}\phi^{T}\phi + 2w^{T}D - 2μ^{T}D$

Move negative terms to the LHS and divide both sides by 2:

$qt^{T}\phi + μ^{T}D = qw^{T}\phi^{T}\phi + w^{T}D$

$qw^{T}\phi^{T}\phi + w^{T}D = qt^{T}\phi + μ^{T}D$

Take the transpose of both sides (note: $D^{T} = D$:

$q\phi^{T}\phi w + Dw = q\phi^{T}t + Dμ$

$(D + q\phi^{T}\phi)w_{Bayes} = q\phi^{T}t + Dμ$



**2. Curve fitting (pt1) The aim is to learn a smooth function from the cloud of points stored in curve_data.txt using Bayesian linear regression models. In all that follows the prior p(w) = N (w|0, δ−1I) is used and q, δ are constants specified by the user.**

1. Plot the data

```{r, echo = FALSE}
curve_data <- read.csv('curve_data.txt', sep = " ")
plot(curve_data)
```

2. Write a function in R, called phix...

```{r}
phix <- function(x, M, basis) {
  # Initialize vector to return
  u <- rep(0, M+1)
  # μs for Gauss
  μs <- rep(0, M+1)
  interval <- 1 / M

  if (basis == 'poly') {
    # evaluate x at each value of p, e.g. x^0, x^1, ..., x^M
    for (p in 0:M) u[p+1] <- x^p
  } else if (basis == 'Gauss') {
    # Populate μs with M intervals in [0,1]
    for (n in 1:M) μs[n+1] = μs[n] + interval
    # Generate Gaussian Kernels
    for (p in 0:M) u[p+1] <- exp((-(x - μs[p+1])^2)/0.1)
  }

  u
}
# Test: round(phix(0.3,4,'poly'), digits = 4) == c(1, 0.3, 0.09, 0.027, 0.0081)
# Test: plot(phix(0.3,10,'Gauss'))
```

3. Write a function in R, called post.params, that takes as input the training data, M, the type of basis, the function phix, $\delta$ and q and returns the parameters of the posterior distribution, $w_{Bayes}$ and $Q$.

```{r}
library(MASS)

post.params <- function(training_data, M, basis_type, phix_func, delta, q) {
  # regularization coefficient
  lambda = delta / q

  # Note: this requires the training data to have an x column
  phi = training_data$x
  
  # Construct a phi matrix that is phix for each value of    M
  
  
  t = training_data$t
  
  # Slide 10 of Bayessian_regression.pdf
  # D = δI
  D = delta # FIXME: CAUSING PROBLEMS ---> delta * diag(ncol(phi))
  # wBayes = (λI + ΦT Φ)−1ΦT t , λ = δ/q
  # FIXME: I got rid of the Identity matrix because it was non-conformable
  w_Bayes = ginv(lambda + t(phi) %*% phi) %*% t(phi) %*% t
  Q = D + q * (t(phi) %*% phi)
  # THIS IS WRONG SOMEHOW
  c(w_Bayes, Q)
}

q = (1/0.1)^2
delta = 2.0
post.params(curve_data, 3, 'poly', 'phix_func', delta, q)
# [1]  -0.4388343 353.8518519
plot(phix(0.3,9,'poly'))

```
