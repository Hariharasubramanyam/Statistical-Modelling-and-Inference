---
title: "Lecture 3 Exercises"
output: html_document
---

**1. For a Bayesian linear regression model with prior on $p(w) = N (w | \mu, D−1)$ and q known, show that $w_{Bayes}$ solves $(D + q\phi^{T}\phi)w_{Bayes} = q\phi^{T}t + D\mu$**

To solve for bayes we take the known posterior probability:

$−2 log p(w|t,X) = −2qt^{T}\phi w + qw^{T}\phi^{T}\phi w + (w − μ)^{T}D(w − μ) + const$

To get the best estimate of w, we take the derivative of the posterior probability and set it equal to zero:

$\partial(−2 log p(w|t, X))/\partial(w)$ = 0

To take the derivative of the first term we use the property that $\partial u^{T}x/\partial x = ??$:

$\partial(−2qt^{T}\phi w)/\partial(w) = -2q\phi^{T}t$

The derivative of the second term uses the propertye $\partial x^{T}Ax/ \partial x = 2Ax$

$\partial q w^{T} \phi^{T} \phi w/\partial w = 2q\phi^{T}\phi w$

The derivative if the third term uses the same:

$\partial (w - \mu)^{T}D(w - \mu) / \partial w = 2D(w - \mu)$

So we have:

$0 = -2q\phi^{T}t + 2q\phi^{T}\phi w + 2D(w - \mu)$

Add the first term to the LHS and divide both sides by 2:

$q\phi^{T}t = q\phi^{T}\phi w + D(w - \mu)$

Expand the last term:

$q\phi^{T}t = q\phi^{T}\phi w + Dw - D\mu$

Gather non-$w$ terms on the LHS:

$q\phi^{T}t + D\mu = (q\phi^{T}\phi + D)w$

$(D + q\phi^{T}\phi)w_{Bayes} = q\phi^{T}t + D\mu$


**2. Curve fitting (pt1) The aim is to learn a smooth function from the cloud of points stored in curve_data.txt using Bayesian linear regression models. In all that follows the prior p(w) = N (w|0, δ−1I) is used and q, δ are constants specified by the user.**

1. Plot the data

```{r, echo = FALSE}
curve_data <- read.csv('curve_data.txt', sep = " ")
plot(curve_data)
```

2. Write a function in R, called phix...

```{r}
phix <- function(x, M, basis) {
  # Initialize vector to return
  u <- rep(0, M+1)
  # Mus for Gauss
  mus <- rep(0, M+1)
  interval <- 1 / M

  if (basis == 'poly') {
    # evaluate x at each value of p, e.g. x^0, x^1, ..., x^M
    for (p in 0:M) u[p+1] <- x^p
  } else if (basis == 'Gauss') {
    # Populate mus with M intervals in [0,1]
    for (n in 1:M) mus[n+1] = mus[n] + interval
    # Generate Gaussian Kernels
    for (p in 0:M) u[p+1] <- exp((-(x - mus[p+1])^2)/0.1)
  }

  u
}
# Test: round(phix(0.3,4,'poly'), digits = 4) == c(1, 0.3, 0.09, 0.027, 0.0081)
# Test: plot(phix(0.3,10,'Gauss'))
```
