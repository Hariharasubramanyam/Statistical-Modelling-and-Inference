---
title: "Statistical Modelling and Inference: Week 3 Prior Modelling"
output: pdf_document
---

#### Prior Modelling

** 1. Extend your program to also learn q from the data. Work with the Normal.Gamma prior with specification** 

  $a = -\frac{1}{2}, b = 0, \mu = 0, D = \delta I$

* Produce a figure that includes posterior draws of the linear predictor for this model

#### 1. Continuation of your work on smooth function estimation with the data in curve_data.txt. Extend your program to also learn q from the data. Work with the Normal-Gamma prior with specification.

**Produce a figure that includes posterior draws of the linear predictor for this model**

```{r, echo = FALSE, message=FALSE, warning=FALSE}
library(metRology)
library(splines)
library(MASS)

phix <- function(x, M, basis_type) {
  phi <- rep(0, M)
  if (basis_type == "poly") {
    for (i in 1:(M)) {
      phi[i] <- x**i
    }
  }
  if (basis_type == "Gauss") {
    phi[1] <- 1
    for (i in 2:(M+1)) {
      phi[i] <- exp(-((x-i/(M))**2)/0.1)
    }
    
  }
  phi
}

post.params <- function(data, M, basis_type, delta, a0, b0, og) {
  N = length(data$x)
  phi = phix(data$x[1], M, basis_type)
  for (i in 2:length(data$x)) {
    phi_ <- phix(data$x[i], M, basis_type)
    phi = rbind(phi, phi_)  
  }

  data.precision <- 1 / (sd(data$t)^2)

  # Estimate parameters using MLE
  mle <- lm(data$t ~ phi)
  mle.params <- mle$coefficients
  mle.var <- sd(mle$fitted.values)^2
  mle.precision <- 1 / mle.var
  mle.fitted.values <- mle$fitted.values
  
  # Estimate parameters using Bayes - claiming to know q
  regularization.D <- delta * diag(ncol(phi))
  bayes.Q <- regularization.D + q * (t(phi) %*% phi)
  bayes.params <- solve(bayes.Q) %*% (q * t(phi) %*% data$t)
  bayes.fitted.values <- phi %*% bayes.params
  bayes.var <- sd(bayes.fitted.values) ^ 2
  bayes.precision <- 1 / bayes.var

  # Estimate parameters using Bayes and sNG - setting 'g' to 1
  g <- og
  bayes.sng.Dprime <- 1/g * (regularization.D + g * (t(phi) %*% phi))
  bayes.sng.aprime <- a0 + N/2
  bayes.sng.K.identity <- diag(nrow = N, ncol = N)
  bayes.sng.K <- g * phi %*% solve(regularization.D + g * t(phi) %*% phi) %*% t(phi)
  bayes.sng.bprime <- b0 + (1/2) * t(data$t) %*% (bayes.sng.K.identity - bayes.sng.K) %*% data$t
  bayes.sng.fitted.values <- bayes.sng.K %*% data$t
  
  return(list(
    mle.fitted.values = mle.fitted.values,
    bayes.fitted.values = bayes.fitted.values,
    bayes.sng.fitted.values = bayes.sng.fitted.values,
    bayes.params = bayes.params,
    bayes.sng.bprime = bayes.sng.bprime,
    bayes.sng.aprime = bayes.sng.aprime,
    bayes.sng.Dprime = bayes.sng.Dprime))
}

M <- 6
delta <- 1
q <- (1/0.1)^2
a0 <- 10
b0 <- 0
basis_type <- 'Gauss'
data <- read.csv('curve_data.txt', sep = ' ')
og <- 1

params <- post.params(data, M, basis_type, delta, a0, b0, og)
plot(c(0,1), c(-1,1), type="n", main="Model Options for Curve Data")
points(data$x, data$t, col = 'chocolate1', pch = 21, bg = 'chocolate1')
lines(predict(splines::interpSpline(data$x, params$mle.fitted.values)), col = 'green')
lines(predict(splines::interpSpline(data$x, params$bayes.fitted.values)), col = 'dodgerblue4')
lines(predict(splines::interpSpline(data$x, params$bayes.sng.fitted.values)), col = 'orange')
legend(0, 0, legend = c("mle", "bayes", "bayes (unknown precision)"), lty=c(1,1), lwd=c(2.5,2.5),col=c("green", "dodgerblue4", "orange"))

test.x <- seq(0,1,1/10)

predict.t.studentsT <- function(data, x, M, basis_type, delta, a, b) {
  bayes.post.params <- post.params(data, M, basis_type, delta, a0, b0, og)

  test.y <- matrix(nrow = length(test.x), ncol = 2)
  dimnames(test.y)[[2]] <- list("test.x", "test.y")
  # create phi(testx) by sending all the test x to phix
  phi.test.x <- matrix(nrow = length(test.x), ncol = M+1)
  for (n in 1:length(test.x)) {
    test.y[n,"test.x"] <- test.x[n]
    phi.test.x[n,] <- c(phix(test.x[n], M, basis_type))
  }

  for (n in 1:nrow(phi.test.x)) {
    xphix <- phi.test.x[n,]

    F <- solve(1 + t(xphix) %*% solve(bayes.post.params$bayes.sng.Dprime) %*% xphix)
    test.y[n,"test.y"] <- rt.scaled(
      n = 1,
      mean = t(xphix) %*% bayes.post.params$bayes.params,
      sd = 1/sqrt(bayes.post.params$bayes.sng.aprime/bayes.post.params$bayes.sng.bprime * F),
      df = bayes.post.params$bayes.sng.aprime * 2)
  }

  return(test.y)
}

plot(data, main="Predict t+1 using student-T distribution")
for (n in 1:100) {
  test.y <- predict.t.studentsT(data, test.x, M, basis_type, delta, a, b)
  lines(predict(splines::interpSpline(test.y[,'test.x'], test.y[,'test.y'])), col = 'darkseagreen') 
}
points(data, col = 'chocolate1', pch = 21, bg = 'chocolate1')

predict.sng.params <- function(test.x) {
  phi = phix(test.x[1], M, basis_type)
  for (i in 2:length(test.x)) {
    phi_ <- phix(test.x[i], M, basis_type)
    phi = rbind(phi, phi_)  
  }

  params <- post.params(data, M, basis_type, delta, a0, b0, og)
  # Sample from gamma with a aprime and b bprime to generate tau (precision)
  tau <- rgamma(1, rate = params$bayes.sng.bprime, shape = params$bayes.sng.aprime)
  # Calculate variance for multivariate normal
  #Dprime <- (1 / tau) * (params$bayes.sng.Dprime + tau * (t(phi) %*% phi))
  Dprime <- params$bayes.sng.Dprime
  # Sample from normal with mean wbayes and variance 1 / lambda (D) * tau
  # Take random draws of our sNG for w
  w.random <- mvrnorm(n = 1, mu = params$bayes.params, Sigma = tau * solve(q * Dprime))

  test.y <- matrix(nrow = length(test.x), ncol = 2)
  dimnames(test.y)[[2]] <- list("test.x", "test.y")
  for (n in 1:nrow(test.y)) {
    test.y[n,'test.x'] <- test.x[n]
    xphix <- phi[n,]

    F <- solve(1 + t(xphix) %*% solve(Dprime) %*% xphix)
    test.y[n,"test.y"] <- rt.scaled(
      n = 1,
      mean = t(xphix) %*% w.random,
      sd = 1/sqrt(params$bayes.sng.aprime/params$bayes.sng.bprime * F),
      df = params$bayes.sng.aprime * 2)
  }
  return(list(predictions = test.y))
}

plot(data, col = 'chocolate1', pch = 21, bg = 'chocolate1', main = 'Predict t+1 using random draws \nof the scaled Normal-Gamma for w,q')
for (n in 1:100) {
  predictions <- predict.sng.params(seq(0,1,1/10))$predictions
  lines(predict(splines::interpSpline(predictions[,'test.x'], predictions[,'test.y'])), col = 'dodgerblue4')
}
points(data, col = 'chocolate1', pch = 21, bg = 'chocolate1')
```

Exploring the sensitvity to delta exposed that a large delta drives the values of the parameters to 0 and thus there is no correlation between x and the fitted values for t. A low delta or zero delta drives a closer fit to the observations.


**2.1 Priors that penalize the L norm of the mean of a Gaussian and MAP estimation**

**1. Suppose for simplicity that a > 0 (a < 0 can be handled in the same way), assume also that $\lambda > 0$ and consider the function**

$f(\mu) = (\mu -a)^{2} + \lambda|\mu|$

Show that $\mu$ is minimised at $(a - \lambda/2)^{+}$ where $x^{+}$ denotes the positive part of x.

We take the derivative and equate to zero to find the $\lambda$ that maximizes the L norm

$\dfrac{\partial f(\mu)}{\partial\mu} = 2(\mu - a)^{2} + \lambda|\mu| = 0$

$\lambda = -2(\mu - a)$

$\mu = a - \dfrac{\lambda}{2}$

And we can check that it is indeed a minimum as the second derivative is > 0:

$\dfrac{\partial}{\partial\mu}\big(\dfrac{\partial f(\mu)}{\partial\mu}\big) = 2 > 0$

**2. Find $\textbf{w}_{MAP}$ under this model in closed form**

$\textbf{w}_{MAP} = arg \ max \ log \ p(w|t) = arg \ max \ log \ p(t|w) + log \ p(w)$ 

$$= \ arg \ max \ log \ \prod_{i=1}^{N}p(t_{n}|w) + log \ e^{-\frac{\delta}{2}\sum_{i = 1}^{M+1}|w_{i}|}$$

$$= \ arg \ max \ \sum_{i=1}^{N} \ log \ N(w_{n}, q^{-1}I) + log \ e^{-\frac{\delta}{2}\sum_{i = 1}^{M+1}|w_{i}|}$$

$$= \ arg \ max \ \sum_{i=1}^{N} \ c \ log \ e^{-\frac{1}{2}q(t_{n} - w)^{T}(t - w)} + log \ e^{-\frac{\delta}{2}\sum_{i = 1}^{M+1}|w_{i}|}$$

$$= \ arg \ max \ \sum_{i=1}^{N} \ -\frac{c}{2}q(t_{n} - w)^{T}(t - w) -\frac{\delta}{2}\sum_{i = 1}^{M+1}|w_{i}|$$

$$\propto \ min \ \sum_{i=1}^{N} \ q(t_{n} - w)^{T}(t - w) + \delta\sum_{i = 1}^{M+1}|w_{i}|$$

$$= \ min \ q\sum_{i=1}^{N}\sum_{i=1}^{M+1} \ (t_{n} - w)^{T}(t - w) + \delta\sum_{i = 1}^{M+1}|w_{i}|$$

Taking the derivative and equating to 0,

$\dfrac{\partial}{\partial w_{i}} = 0$ $$-2q\sum_{i=1}^{N}(t_{ni} - w_{i}) + \delta = 0$$

$$-Nw_{i} + \sum_{i=1}^{N}t_{ni} = \frac{\delta}{2}q^{-1}$$

$$w_{i} = \frac{1}{N}(\sum_{i=1}^{N}t_{ni} - \frac{\delta}{2}q^{-1})$$
